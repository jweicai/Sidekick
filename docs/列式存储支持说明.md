# Sidekick 列式存储格式支持说明

## 概述

Sidekick 现已支持读取 **Apache Parquet** 列式存储格式文件，**无需任何外部依赖**！

通过集成 DuckDB 数据库引擎，Sidekick 可以直接读取 Parquet 文件，提供与 CSV、JSON、XLSX 相同的流畅体验。

---

## 支持的格式

### Apache Parquet ✅

**文件扩展名：** `.parquet`

**特点：**
- 列式存储，高效压缩
- 支持复杂嵌套数据结构
- 广泛用于 Spark、Hive、Presto 等大数据生态
- 支持多种压缩算法（Snappy、GZIP、LZO、Brotli、ZSTD 等）

**使用场景：**
- 大数据分析
- 数据湖存储
- ETL 数据处理
- 机器学习数据集
- 数据仓库

---

## 技术实现

### 内置 DuckDB 引擎

Sidekick 集成了 **DuckDB** - 一个高性能的嵌入式分析数据库：

- ✅ **零外部依赖** - DuckDB 静态链接到应用中
- ✅ **原生 Parquet 支持** - 完整支持 Parquet 规范
- ✅ **高性能** - 专为分析查询优化
- ✅ **开箱即用** - 无需安装任何额外软件

### 优势

1. **用户友好**
   - 无需安装 Python、PyArrow 或其他工具
   - 双击应用即可使用
   - 与其他格式体验一致

2. **功能完整**
   - 支持所有 Parquet 压缩格式
   - 支持嵌套数据结构
   - 支持大文件（流式读取）

3. **性能优秀**
   - DuckDB 针对分析查询优化
   - 列式存储充分利用
   - 高效的内存管理

---

## 使用方法

### 1. 导入 Parquet 文件

与其他格式完全相同，有三种方式：

**方法 1 - 拖拽导入：**
- 直接将 `.parquet` 文件拖入 Sidekick 窗口

**方法 2 - 文件选择器：**
- 点击"选择文件"按钮
- 在文件选择器中选择 Parquet 文件

**方法 3 - 快捷键：**
- 按 `⌘+O` 打开文件选择器

### 2. 查看数据

导入成功后，数据会自动加载：
- 在左侧表列表中看到导入的表
- 点击表名查看数据预览
- 所有列和数据类型自动识别

### 3. SQL 查询

使用标准 SQL 查询 Parquet 数据：

```sql
-- 查看所有数据
SELECT * FROM your_table;

-- 筛选数据
SELECT * FROM your_table 
WHERE column_name > 100;

-- 聚合分析
SELECT category, COUNT(*), AVG(value)
FROM your_table
GROUP BY category;

-- 多表关联
SELECT a.*, b.name
FROM table_a a
JOIN table_b b ON a.id = b.id;

-- 复杂查询
SELECT 
    department,
    COUNT(*) as employee_count,
    AVG(salary) as avg_salary,
    MAX(salary) as max_salary
FROM employees
WHERE age > 25
GROUP BY department
HAVING COUNT(*) > 5
ORDER BY avg_salary DESC;
```

### 4. 导出数据

查询结果可以导出为：
- ✅ CSV 格式
- ✅ JSON 格式
- ✅ SQL INSERT 语句

---

## 性能特性

### 文件大小支持

- **小文件（< 10 MB）**：秒级加载
- **中等文件（10-100 MB）**：快速加载
- **大文件（100 MB - 1 GB）**：流式读取，内存友好
- **超大文件（> 1 GB）**：建议先采样或筛选

### 性能优化

DuckDB 自动进行以下优化：

1. **列式读取** - 只读取查询需要的列
2. **谓词下推** - 在读取时就进行筛选
3. **压缩感知** - 直接在压缩数据上操作
4. **并行处理** - 多核并行读取和查询

### 内存使用

- DuckDB 使用流式处理
- 不会一次性加载整个文件到内存
- 适合处理大于可用内存的文件

---

## 数据类型支持

### 基本类型

| Parquet 类型 | Sidekick 显示 |
|-------------|--------------|
| INT32, INT64 | 整数 |
| FLOAT, DOUBLE | 小数 |
| BOOLEAN | true/false |
| STRING, BINARY | 文本 |
| DATE | 日期 (YYYY-MM-DD) |
| TIMESTAMP | 时间戳 |
| DECIMAL | 精确小数 |

### 复杂类型

| Parquet 类型 | Sidekick 显示 |
|-------------|--------------|
| LIST | 数组（逗号分隔） |
| STRUCT | 结构体（键值对） |
| MAP | 映射（键值对） |

---

## 示例场景

### 场景 1：分析 Spark 输出

```sql
-- 假设你有 Spark 生成的 Parquet 文件
-- 直接拖入 Sidekick 即可查询

SELECT 
    date,
    SUM(revenue) as total_revenue,
    COUNT(DISTINCT user_id) as unique_users
FROM spark_output
WHERE date >= '2025-01-01'
GROUP BY date
ORDER BY date;
```

### 场景 2：数据质量检查

```sql
-- 检查空值
SELECT 
    COUNT(*) as total_rows,
    COUNT(column1) as column1_non_null,
    COUNT(column2) as column2_non_null
FROM data_table;

-- 检查重复
SELECT id, COUNT(*) as count
FROM data_table
GROUP BY id
HAVING COUNT(*) > 1;
```

### 场景 3：快速数据探索

```sql
-- 查看数据分布
SELECT 
    category,
    MIN(value) as min_value,
    MAX(value) as max_value,
    AVG(value) as avg_value,
    COUNT(*) as count
FROM dataset
GROUP BY category;
```

---

## 常见问题

### Q1: 支持哪些压缩格式？

**A:** DuckDB 支持所有常见的 Parquet 压缩格式：
- Snappy（默认）
- GZIP
- Brotli
- ZSTD
- LZ4
- 无压缩

### Q2: 可以读取分区的 Parquet 文件吗？

**A:** 当前版本只支持单个 Parquet 文件。如果你有分区目录（如 Hive 分区），需要：
1. 合并为单个文件，或
2. 分别导入每个分区文件

### Q3: 支持嵌套数据结构吗？

**A:** 支持！嵌套的 LIST、STRUCT、MAP 类型会被展平显示：
- LIST: 显示为逗号分隔的字符串
- STRUCT: 显示为键值对
- MAP: 显示为键值对

### Q4: 文件太大怎么办？

**A:** 几种处理方式：

1. **使用 SQL 筛选**
   ```sql
   -- 只查询需要的列和行
   SELECT col1, col2 FROM large_table WHERE date = '2025-01-14';
   ```

2. **导出采样数据**
   ```sql
   -- 随机采样 1000 行
   SELECT * FROM large_table LIMIT 1000;
   ```

3. **使用外部工具预处理**
   - 使用 DuckDB CLI 先筛选
   - 使用 Spark/Pandas 采样

### Q5: 读取速度慢怎么办？

**可能原因：**
- 文件过大
- 复杂的嵌套结构
- 磁盘 I/O 瓶颈

**优化建议：**
- 只查询需要的列
- 使用 WHERE 条件筛选
- 将文件放在 SSD 上

### Q6: 中文显示正常吗？

**A:** 完全正常！Parquet 使用 UTF-8 编码，中文、日文、韩文等都能正确显示。

---

## 与其他工具对比

| 工具 | 优势 | 劣势 |
|-----|------|------|
| **Sidekick** | 简单易用、无需配置、SQL 查询 | 不支持写入 Parquet |
| **DBeaver** | 功能强大、支持多种数据库 | 配置复杂、体积大 |
| **Pandas** | 编程灵活、数据处理强大 | 需要写代码、内存占用大 |
| **Spark** | 处理超大数据、分布式 | 配置复杂、启动慢 |
| **Excel** | 熟悉的界面 | 不支持 Parquet |

---

## 技术细节

### DuckDB 集成

Sidekick 使用 DuckDB 的以下特性：

1. **read_parquet() 函数**
   ```sql
   CREATE TABLE data AS 
   SELECT * FROM read_parquet('file.parquet');
   ```

2. **自动类型推断**
   - DuckDB 自动识别 Parquet 的 schema
   - 正确映射所有数据类型

3. **内存管理**
   - 流式处理大文件
   - 智能缓存常用数据

### 应用体积

- 集成 DuckDB 后，应用大小约 **10 MB**
- 相比其他方案（需要 Python + PyArrow），这是最小的

### 兼容性

- ✅ macOS 14.0+
- ✅ Apple Silicon (M1/M2/M3)
- ✅ Intel Mac

---

## 未来计划

### 即将支持

- [ ] 导出为 Parquet 格式
- [ ] 支持分区文件夹
- [ ] 支持远程文件（S3、HTTP）
- [ ] 更好的嵌套数据展示

### 考虑中

- [ ] ORC 格式支持
- [ ] Avro 格式支持
- [ ] Arrow IPC 格式支持

---

## 相关资源

### 官方文档

- [Apache Parquet](https://parquet.apache.org/)
- [DuckDB Documentation](https://duckdb.org/docs/)
- [DuckDB Parquet Support](https://duckdb.org/docs/data/parquet)

### 学习资源

- [Parquet 格式详解](https://parquet.apache.org/docs/)
- [列式存储原理](https://en.wikipedia.org/wiki/Column-oriented_DBMS)
- [DuckDB 性能优化](https://duckdb.org/docs/guides/performance)

### 工具推荐

- [DuckDB CLI](https://duckdb.org/docs/installation/) - 命令行工具
- [parquet-tools](https://pypi.org/project/parquet-tools/) - Python 工具
- [Apache Arrow](https://arrow.apache.org/) - 数据处理框架

---

## 示例数据

### 创建测试文件

如果你想创建测试 Parquet 文件，可以使用 Python：

```python
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

# 创建示例数据
df = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'name': ['张三', '李四', '王五', '赵六', '钱七'],
    'age': [25, 30, 35, 40, 45],
    'city': ['北京', '上海', '深圳', '广州', '杭州'],
    'salary': [50000, 60000, 70000, 80000, 90000]
})

# 写入 Parquet 文件
table = pa.Table.from_pandas(df)
pq.write_table(table, 'test.parquet')
```

或者使用 DuckDB CLI：

```sql
-- 从 CSV 创建 Parquet
COPY (SELECT * FROM 'data.csv') 
TO 'data.parquet' (FORMAT PARQUET);
```

---

## 反馈和支持

如果您在使用 Parquet 功能时遇到问题：

1. 检查文件是否为有效的 Parquet 格式
2. 尝试使用其他工具验证文件
3. 查看应用日志获取详细错误信息
4. 提交 Issue 到 GitHub 仓库

---

**更新日期：** 2025-01-14  
**版本：** 2.0.0 (DuckDB 集成版)  
**技术栈：** Swift + DuckDB 1.1.3

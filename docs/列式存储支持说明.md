# Sidekick 列式存储格式支持说明

## 概述

Sidekick 现已支持读取列式存储格式文件：
- **Parquet** - Apache Parquet 列式存储格式
- **ORC** - Optimized Row Columnar 列式存储格式

这些格式在大数据处理中非常常见，特别是在 Hadoop、Spark、Hive 等生态系统中。

---

## 支持的格式

### 1. Apache Parquet

**文件扩展名：** `.parquet`

**特点：**
- 列式存储，高效压缩
- 支持复杂嵌套数据结构
- 广泛用于 Spark、Hive、Presto 等
- 支持多种压缩算法（Snappy、GZIP、LZO 等）

**使用场景：**
- 大数据分析
- 数据湖存储
- ETL 数据处理
- 机器学习数据集

### 2. Apache ORC

**文件扩展名：** `.orc`

**特点：**
- 列式存储，针对 Hive 优化
- 内置索引和统计信息
- 高效的压缩和编码
- 支持 ACID 事务

**使用场景：**
- Hive 数据仓库
- 大数据批处理
- 数据归档
- 分析型查询

---

## 环境要求

### 必需软件

1. **Python 3**
   - 版本要求：Python 3.7 或更高
   - 检查是否已安装：`python3 --version`

2. **PyArrow 库**
   - Apache Arrow 的 Python 实现
   - 提供 Parquet 和 ORC 读取支持

### 安装步骤

#### 1. 安装 Python 3

**方法 1 - 使用 Homebrew（推荐）：**
```bash
brew install python3
```

**方法 2 - 从官网下载：**
访问 [https://www.python.org/downloads/](https://www.python.org/downloads/) 下载安装包

#### 2. 安装 PyArrow

```bash
# 使用 pip3 安装
pip3 install pyarrow

# 或者使用 python3 -m pip
python3 -m pip install pyarrow
```

**验证安装：**
```bash
python3 -c "import pyarrow; print(pyarrow.__version__)"
```

如果输出版本号（如 `15.0.0`），说明安装成功。

---

## 使用方法

### 1. 导入文件

与其他格式一样，有三种方式导入 Parquet/ORC 文件：

**方法 1 - 拖拽导入：**
- 直接将 `.parquet` 或 `.orc` 文件拖入 Sidekick 窗口

**方法 2 - 文件选择器：**
- 点击"选择文件"按钮
- 在文件选择器中选择 Parquet 或 ORC 文件

**方法 3 - 快捷键：**
- 按 `⌘+O` 打开文件选择器

### 2. 查看数据

导入成功后，数据会自动加载到内存数据库中，您可以：
- 在左侧表列表中看到导入的表
- 点击表名查看数据预览
- 使用 SQL 查询数据

### 3. SQL 查询

```sql
-- 查看所有数据
SELECT * FROM your_table;

-- 筛选数据
SELECT * FROM your_table WHERE column_name > 100;

-- 聚合分析
SELECT category, COUNT(*), AVG(value)
FROM your_table
GROUP BY category;

-- 多表关联
SELECT a.*, b.name
FROM table_a a
JOIN table_b b ON a.id = b.id;
```

### 4. 导出数据

查询结果可以导出为：
- CSV 格式
- JSON 格式
- SQL INSERT 语句

---

## 工作原理

### 技术实现

Sidekick 使用以下技术栈读取列式存储文件：

1. **Python PyArrow**
   - 使用 PyArrow 库读取 Parquet/ORC 文件
   - 将数据转换为 CSV 格式

2. **临时转换**
   - 创建临时 Python 脚本
   - 执行脚本读取文件并输出 CSV
   - 使用 CSVLoader 解析 CSV 数据

3. **内存数据库**
   - 将数据加载到 SQLite 内存数据库
   - 支持标准 SQL 查询

### 数据流程

```
Parquet/ORC 文件
    ↓
Python PyArrow 读取
    ↓
转换为 CSV 格式
    ↓
CSVLoader 解析
    ↓
DataFrame 数据结构
    ↓
SQLite 内存数据库
    ↓
SQL 查询和分析
```

---

## 性能考虑

### 文件大小限制

- **推荐大小：** < 100 MB
- **最大支持：** 取决于可用内存
- **大文件处理：** 建议先使用其他工具进行采样或筛选

### 性能优化建议

1. **采样大文件**
   ```python
   # 使用 Python 预先采样
   import pyarrow.parquet as pq
   
   table = pq.read_table('large_file.parquet')
   sample = table.slice(0, 10000)  # 取前 10000 行
   pq.write_table(sample, 'sample.parquet')
   ```

2. **选择特定列**
   ```python
   # 只读取需要的列
   table = pq.read_table('file.parquet', columns=['col1', 'col2'])
   ```

3. **使用筛选条件**
   ```python
   # 使用 PyArrow 的筛选功能
   import pyarrow.dataset as ds
   
   dataset = ds.dataset('file.parquet')
   filtered = dataset.to_table(filter=ds.field('value') > 100)
   ```

---

## 常见问题

### Q1: 提示"未找到 Python 3"

**解决方案：**
1. 确认 Python 3 已安装：`python3 --version`
2. 如果未安装，使用 Homebrew 安装：`brew install python3`
3. 确保 Python 3 在系统 PATH 中

### Q2: 提示"未安装 PyArrow 库"

**解决方案：**
```bash
# 安装 PyArrow
pip3 install pyarrow

# 如果遇到权限问题
pip3 install --user pyarrow

# 或者使用 sudo（不推荐）
sudo pip3 install pyarrow
```

### Q3: 读取文件失败

**可能原因：**
1. 文件损坏或格式不正确
2. PyArrow 版本过旧
3. 文件使用了不支持的压缩算法

**解决方案：**
```bash
# 升级 PyArrow 到最新版本
pip3 install --upgrade pyarrow

# 验证文件是否可读
python3 -c "import pyarrow.parquet as pq; pq.read_table('your_file.parquet')"
```

### Q4: 文件太大，加载很慢

**解决方案：**
1. 使用 Python 预先采样数据
2. 只读取需要的列
3. 使用筛选条件减少数据量
4. 考虑使用专业的大数据工具（如 DBeaver、DataGrip）

### Q5: 中文乱码问题

**解决方案：**
- Parquet 和 ORC 格式本身支持 UTF-8 编码
- 如果出现乱码，可能是原始数据编码问题
- 检查数据源的编码设置

---

## 命令行工具

如果您需要在命令行中查看 Parquet/ORC 文件，可以使用以下工具：

### Parquet Tools

```bash
# 安装
pip3 install parquet-tools

# 查看文件信息
parquet-tools show file.parquet

# 查看 schema
parquet-tools schema file.parquet

# 导出为 CSV
parquet-tools csv file.parquet > output.csv
```

### PyArrow CLI

```bash
# 查看文件内容
python3 -c "import pyarrow.parquet as pq; print(pq.read_table('file.parquet'))"

# 查看 schema
python3 -c "import pyarrow.parquet as pq; print(pq.read_schema('file.parquet'))"
```

---

## 示例数据

### 创建测试 Parquet 文件

```python
import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd

# 创建示例数据
df = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],
    'age': [25, 30, 35, 40, 45],
    'salary': [50000, 60000, 70000, 80000, 90000]
})

# 转换为 Arrow Table
table = pa.Table.from_pandas(df)

# 写入 Parquet 文件
pq.write_table(table, 'sample.parquet')
```

### 创建测试 ORC 文件

```python
import pyarrow as pa
import pyarrow.orc as orc
import pandas as pd

# 创建示例数据
df = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'product': ['A', 'B', 'C', 'D', 'E'],
    'quantity': [10, 20, 30, 40, 50],
    'price': [100.0, 200.0, 300.0, 400.0, 500.0]
})

# 转换为 Arrow Table
table = pa.Table.from_pandas(df)

# 写入 ORC 文件
orc.write_table(table, 'sample.orc')
```

---

## 技术限制

### 当前限制

1. **不支持嵌套结构**
   - 复杂的嵌套数据结构会被展平
   - Map、List、Struct 类型可能显示为字符串

2. **不支持分区文件**
   - 不支持读取分区的 Parquet/ORC 目录
   - 只能读取单个文件

3. **不支持写入**
   - 目前只支持读取，不支持写入 Parquet/ORC 格式
   - 可以导出为 CSV、JSON 格式

4. **依赖外部工具**
   - 需要安装 Python 和 PyArrow
   - 无法在没有 Python 的环境中使用

### 未来改进

- [ ] 支持嵌套数据结构的展示
- [ ] 支持分区文件读取
- [ ] 支持导出为 Parquet/ORC 格式
- [ ] 内置 PyArrow 支持（无需外部依赖）
- [ ] 支持流式读取大文件
- [ ] 支持远程文件（S3、HDFS）

---

## 相关资源

### 官方文档

- [Apache Parquet](https://parquet.apache.org/)
- [Apache ORC](https://orc.apache.org/)
- [PyArrow Documentation](https://arrow.apache.org/docs/python/)

### 学习资源

- [Parquet 格式详解](https://parquet.apache.org/docs/)
- [ORC 格式详解](https://orc.apache.org/docs/)
- [列式存储原理](https://en.wikipedia.org/wiki/Column-oriented_DBMS)

### 相关工具

- [DBeaver](https://dbeaver.io/) - 支持 Parquet 的数据库工具
- [Apache Drill](https://drill.apache.org/) - 支持 Parquet/ORC 的查询引擎
- [Presto](https://prestodb.io/) - 分布式 SQL 查询引擎

---

## 反馈和支持

如果您在使用列式存储格式时遇到问题，请：

1. 检查本文档的"常见问题"部分
2. 确认 Python 和 PyArrow 已正确安装
3. 验证文件格式是否正确
4. 提交 Issue 到 GitHub 仓库

---

**更新日期：** 2025-01-14  
**版本：** 1.0.0
